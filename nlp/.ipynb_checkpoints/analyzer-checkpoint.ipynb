{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from importlib import util as importlibutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PAGE_SIZE = 100\n",
    "MINUTES_IN_HOUR = 60\n",
    "SECONDS_IN_MINUTE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable requests cache\n",
    "if importlibutil.find_spec(\"requests_cache\"):\n",
    "    import requests_cache\n",
    "    requests_cache.install_cache('.requests_cache', expire_after=SECONDS_IN_MINUTE * MINUTES_IN_HOUR * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "update_interval = 360  # minutes\n",
    "countries = ['in', 'us', 'gb', 'au', 'ca', 'nz'] # 'in' ['in', 'us', 'gb', 'au', 'ca', 'nz']\n",
    "categories = {\n",
    "    'business',\n",
    "    'entertainment',\n",
    "    'general',\n",
    "    'health',\n",
    "    'science',\n",
    "    'sports',\n",
    "    'technology'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keys\n",
    "NEWSAPI_ORG_KEY = os.environ.get('NEWSAPI_ORG_KEY')\n",
    "if not NEWSAPI_ORG_KEY:  # TODO(devansh): Remove\n",
    "    NEWSAPI_ORG_KEY = 'ea0f26bbe06b44b898f0f0a80af00c7d'\n",
    "MONGODB_URL = os.environ.get('MONGODB_URL')\n",
    "if not MONGODB_URL:\n",
    "    MONGODB_URL = 'mongodb://localhost:27017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FetchNews(newsapi):\n",
    "    news = {category: [] for category in categories}\n",
    "    for category in categories:\n",
    "        articles = []\n",
    "        keep_downloading = True\n",
    "        cindex = 0\n",
    "        while keep_downloading:\n",
    "            print('newsapi.get_top_headlines(category=', category,\n",
    "                  ', page=1', 'country='+countries[cindex] if countries != [] else '', \n",
    "                  ', page_size=', PAGE_SIZE, ')')\n",
    "            r = None\n",
    "            if countries == []:\n",
    "                r = newsapi.get_top_headlines(category=category,\n",
    "                                              page=1,\n",
    "                                              page_size=PAGE_SIZE)\n",
    "                if (r['status'] == 'ok'):\n",
    "                    news[category] = r['articles']\n",
    "                keep_downloading = False\n",
    "            else:\n",
    "                r = newsapi.get_top_headlines(country=countries[cindex],\n",
    "                                              category=category,\n",
    "                                              page=1,\n",
    "                                              page_size=PAGE_SIZE)\n",
    "                for c in r['articles']:\n",
    "                    c['country'] = countries[cindex]\n",
    "                # print(r['articles'])\n",
    "                news[category].extend(r['articles'])\n",
    "                cindex += 1\n",
    "                keep_downloading = cindex < len(countries)\n",
    "\n",
    "    for category_news in news:\n",
    "        print(category_news, len(news[category_news]))\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeReadyForImport(data):\n",
    "    def Transform(category, article):\n",
    "        article['category'] = category\n",
    "        return article\n",
    "    r = []\n",
    "    for category in categories:\n",
    "        r.extend(list(map(lambda x: Transform(category, x), data[category])))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsapi.get_top_headlines(category= entertainment , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= entertainment , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= entertainment , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= entertainment , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= entertainment , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= entertainment , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= sports , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= health , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= general , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= business , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= science , page=1 country=nz , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=in , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=us , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=gb , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=au , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=ca , page_size= 100 )\n",
      "newsapi.get_top_headlines(category= technology , page=1 country=nz , page_size= 100 )\n",
      "entertainment 383\n",
      "sports 386\n",
      "health 329\n",
      "general 228\n",
      "business 344\n",
      "science 235\n",
      "technology 420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f44a2307e08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init newsapi\n",
    "newsapi = NewsApiClient(api_key=NEWSAPI_ORG_KEY)\n",
    "\n",
    "# Init Mongo\n",
    "mongo_client = MongoClient(MONGODB_URL)\n",
    "db = mongo_client['feed']\n",
    "article = db.article\n",
    "\n",
    "for row in article.find().sort('_id', -1).limit(1):\n",
    "    diff = datetime.now(timezone.utc) - row['_id'].generation_time\n",
    "    (m, s) = divmod(diff.total_seconds(), MINUTES_IN_HOUR)\n",
    "    if (m <= update_interval):\n",
    "        print('Not fetching/updating, last update:', m, 'minutes ago')\n",
    "        exit(0)\n",
    "news = FetchNews(newsapi)\n",
    "news = MakeReadyForImport(news)\n",
    "\n",
    "# Drop collection articles\n",
    "article.drop()\n",
    "article.insert_many(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractText(data):\n",
    "    def Prune(s):\n",
    "        if not s:\n",
    "            return ''\n",
    "        return s\n",
    "    def StripFromEnd(s):\n",
    "        return re.sub(r'\\[.+\\]$', '', Prune(s))\n",
    "    def Special(pre, text):\n",
    "        return pre + '_' + text\n",
    "    def ExtractItem(item):\n",
    "        return ' '.join([Prune(item['title']), Prune(item['description']), StripFromEnd(item['content']),\n",
    "                         Special('country', item['country']), Special('category', item['category'])])\n",
    "    result = [''] * len(data)\n",
    "    for i, item in enumerate(data):\n",
    "#         print(i)\n",
    "        result[i] = ExtractItem(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_news = ExtractText(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_features=50, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.fit_transform(extracted_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x[2].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99831514, 2.76545333, 3.2671084 , 3.79607828, 3.80314544,\n",
       "       4.0069732 , 4.59302225, 2.90836092, 2.80126278, 3.31818333,\n",
       "       2.95281268, 3.28807353, 2.79348064, 2.7092725 , 3.87670801,\n",
       "       3.58711936, 2.69044841, 2.6788608 , 2.91417489, 2.66513061,\n",
       "       2.98671423, 2.849272  , 3.6893103 , 4.23011676, 3.8615562 ,\n",
       "       3.8615562 , 3.75469306, 4.05142497, 4.05142497, 3.83192441,\n",
       "       3.59861374, 4.30925408, 3.7346255 , 4.33306473, 2.3533104 ,\n",
       "       2.88260842, 3.91562343, 3.65203891, 3.56451953, 3.79607828,\n",
       "       3.95611479, 3.74795903, 4.24104583, 3.89987507, 3.49440996,\n",
       "       3.31382602, 3.96441359, 3.57012178, 2.46404677, 3.47890577])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AddTfIdfScores(data):\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
